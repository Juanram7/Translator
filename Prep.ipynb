{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a0367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\juan\\anaconda3\\lib\\site-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\juan\\anaconda3\\lib\\site-packages (from -r MT-Preparation/requirements.txt (line 2)) (1.4.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\juan\\anaconda3\\lib\\site-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\juan\\anaconda3\\lib\\site-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\juan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r MT-Preparation/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c02e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, training a SentencepPiece model for the Source finished successfully!\n",
      "Done, training a SentencepPiece model for the Target finished successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=src.txt --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: src.txt\n",
      "  input_format: \n",
      "  model_prefix: source\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â�‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: src.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1687 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=66897\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9566% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=65\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999566\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1687 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=42501\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 2634 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1687\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 1760\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 1760 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1249 obj=13.6335 num_tokens=5612 num_tokens/piece=4.49319\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1060 obj=10.6945 num_tokens=5641 num_tokens/piece=5.3217\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tgt.txt --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tgt.txt\n",
      "  input_format: \n",
      "  model_prefix: target\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â�‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: tgt.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1418 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=57150\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9563% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=72\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999563\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1418 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=33902\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 7445 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1418\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2534\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 2534 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2318 obj=11.5417 num_tokens=5444 num_tokens/piece=2.34858\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2107 obj=9.3755 num_tokens=5517 num_tokens/piece=2.61841\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n"
     ]
    }
   ],
   "source": [
    "!python MT-Preparation/subwording/1-train_unigram.py src.txt tgt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cc8b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is EA41-6B31\n",
      "\n",
      " Directory of C:\\Users\\Juan\\nmt\n",
      "\n",
      "09/17/2023  03:16 AM    <DIR>          .\n",
      "09/17/2023  03:16 AM    <DIR>          ..\n",
      "09/17/2023  03:13 AM    <DIR>          .ipynb_checkpoints\n",
      "09/17/2023  03:13 AM    <DIR>          MT-Preparation\n",
      "09/17/2023  03:16 AM             2,526 Prep.ipynb\n",
      "09/17/2023  03:16 AM           253,146 source.model\n",
      "09/17/2023  03:16 AM            17,523 source.vocab\n",
      "09/17/2023  03:12 AM            78,314 src.txt\n",
      "09/17/2023  03:16 AM           271,242 target.model\n",
      "09/17/2023  03:16 AM            37,617 target.vocab\n",
      "09/17/2023  03:12 AM            60,255 tgt.txt\n",
      "09/17/2023  02:07 AM             2,893 translate.py\n",
      "               8 File(s)        723,516 bytes\n",
      "               4 Dir(s)  105,615,986,688 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870fa393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Model: source.model\n",
      "Target Model: target.model\n",
      "Source Dataset: src.txt\n",
      "Target Dataset: tgt.txt\n",
      "Done subwording the source file! Output: src.txt.subword\n",
      "Done subwording the target file! Output: tgt.txt.subword\n"
     ]
    }
   ],
   "source": [
    "!python MT-Preparation/subwording/2-subword.py source.model target.model src.txt tgt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e576d85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHEJAB’ HUM CHMANHK’ON POTX’ IX B’OJ YAQ’B’ILXA SYA’TAJIL CH’EK’ YINH SQ’INAL HEB’ YA’ IX.\n",
      "\n",
      "CHEJAB’ MUNIL 22-2008 SB’ISIL.\n",
      "\n",
      "NHAHIL YULPOPB’AL YET TX’OTX’ XE’Q’AQ’.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"src.txt\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a6dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEY CONTRA EL FEMICIDIO Y OTRAS FORMAS DE VIOLENCIA CONTRA LA MUJER.\n",
      "\n",
      "DECRETO NÚMERO 22-2008.\n",
      "\n",
      "EL CONGRESO DE LA REPÚBLICA DE GUATEMALA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"tgt.txt\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89028b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (1935, 2)\n",
      "--- Empty Cells Deleted --> Rows: 1225\n",
      "--- Wrote Files\n",
      "Done!\n",
      "Output files\n",
      "src.txt.subword.train\n",
      "tgt.txt.subword.train\n",
      "src.txt.subword.dev\n",
      "tgt.txt.subword.dev\n",
      "src.txt.subword.test\n",
      "tgt.txt.subword.test\n"
     ]
    }
   ],
   "source": [
    "!python MT-Preparation/train_dev_split/train_dev_test_split.py 500 500 src.txt.subword tgt.txt.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bda0b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁CHEJ AB ’ ▁MUNIL ▁ 2 2 - 2 0 0 8 ▁S B ’ ISIL .\n",
      "\n",
      "▁sb ’ ah , ▁haxkam i ▁chyoche ▁ta ▁ay ▁naj ▁sk ’ ulal ▁yinh\n",
      "\n",
      "▁H AX KA MI :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"src.txt.subword.train\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47062b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁DECRET O ▁N ÚM ERO ▁ 2 2 - 2 0 0 8 .\n",
      "\n",
      "▁organiza ▁para ▁protege r ▁a ▁las ▁personas ▁y ▁a ▁la\n",
      "\n",
      "▁CONSIDERANDO :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"tgt.txt.subword.train\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477c8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
